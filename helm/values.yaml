# NIM Deployment Configuration
# Meta Llama 3.1 8B Instruct on OKE

replicaCount: 1

image:
  registry: nvcr.io
  repository: nim/meta/llama-3.1-8b-instruct
  tag: "latest"
  pullPolicy: IfNotPresent

imagePullSecrets:
  - name: ngc-secret

ngc:
  apiKey: "<YOUR_NGC_API_KEY>"
  registry: nvcr.io
  username: "$oauthtoken"

resources:
  # Dynamic resource allocation based on model requirements
  # NVIDIA NIM Requirements: 40GB RAM minimum, 90GB recommended
  # VM.GPU.A10.4 node provides 960GB RAM (4x 240GB, exceeds requirements)
  # Cost: ~$12.44/hour (4x NVIDIA A10 GPUs)
  limits:
    nvidia.com/gpu: 4
    memory: "{{ .Values.model.memoryLimit | default \"128Gi\" }}"
    cpu: "{{ .Values.model.cpuLimit | default \"32\" }}"
  requests:
    nvidia.com/gpu: 4
    memory: "{{ .Values.model.memoryRequirement | default \"96Gi\" }}"
    cpu: "{{ .Values.model.cpuRequirement | default \"16\" }}"

# Model-specific resource requirements (dynamic allocation)
model:
  name: "meta/llama-3.1-8b-instruct"
  cpuRequirement: "16"       # Dynamic CPU requirement (VM.GPU.A10.4)
  memoryRequirement: "96Gi"  # Dynamic memory requirement (VM.GPU.A10.4)
  cpuLimit: "32"             # Dynamic CPU limit (VM.GPU.A10.4)
  memoryLimit: "128Gi"       # Dynamic memory limit (VM.GPU.A10.4)
  sizeGB: 50                 # Model size for cache planning

service:
  type: LoadBalancer
  port: 8000
  targetPort: 8000
  externalTrafficPolicy: Local
  annotations:
    service.beta.kubernetes.io/oci-load-balancer-shape: "flexible"
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: "10"
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: "10"

env:
  - name: NIM_CACHE_PATH
    value: "/model-cache"
  - name: NIM_MODEL_PROFILE
    value: "auto"
  - name: NGC_API_KEY
    valueFrom:
      secretKeyRef:
        name: nvidia-nim-ngc-api
        key: NGC_API_KEY

persistence:
  enabled: true
  storageClass: "oci-bv"
  accessMode: ReadWriteOnce
  size: 200Gi  # Increased from 50Gi for model caching
  mountPath: /model-cache

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  # seccompProfile: disabled - REQUIRED for NIM GPU compatibility
  # NIM requires GPU syscalls that are blocked by RuntimeDefault seccomp profile
  # Custom seccomp profiles would need to allow GPU-specific syscalls

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false  # NIM requires writable filesystem for temp files and cache

nodeSelector:
  nvidia.com/gpu.present: "true"  # Flexible GPU selection (any NVIDIA GPU)
  # VM.GPU.A10.4 provides 4x NVIDIA A10 GPUs (~$12.44/hour)

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: nvidia.com/gpu.present
              operator: In
              values:
                - "true"

topologySpreadConstraints:
  enabled: false  # DISABLED for single-zone development/testing
  maxSkew: 1
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: ScheduleAnyway  # Allow scheduling even if constraints can't be met
  # TODO: Enable for production multi-zone deployments

readinessProbe:
  httpGet:
    path: /v1/health/ready
    port: 8000
  initialDelaySeconds: 15  # Reduced from 30s for faster deployment
  periodSeconds: 5         # Reduced from 10s for quicker detection
  timeoutSeconds: 3        # Reduced from 5s
  failureThreshold: 6      # Increased to compensate for faster checks

livenessProbe:
  httpGet:
    path: /v1/health/live
    port: 8000
  initialDelaySeconds: 45  # Reduced from 60s
  periodSeconds: 20        # Reduced from 30s
  timeoutSeconds: 5        # Reduced from 10s
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /v1/health/startup
    port: 8000
  initialDelaySeconds: 5   # Reduced from 10s
  periodSeconds: 5         # Reduced from 10s for faster startup detection
  timeoutSeconds: 3        # Reduced from 5s
  failureThreshold: 36     # Increased to maintain same total timeout (3min)

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8000"
  prometheus.io/path: "/metrics"

serviceAccount:
  create: true
  annotations:
    oci.oraclecloud.com/principal-type: "instance"
  name: "nim-service-account"

labels:
  app.kubernetes.io/name: nvidia-nim
  app.kubernetes.io/component: inference
  app.kubernetes.io/part-of: nim-deployment

costOptimization:
  scheduleDowntime: false

