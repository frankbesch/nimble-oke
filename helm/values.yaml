# NVIDIA NIM on OKE - Configuration Values
# Meta Llama 3.1 8B Instruct Model

# Deployment configuration
replicaCount: 1  # Start with 1 for cost optimization

image:
  # NVIDIA NGC Container Registry
  registry: nvcr.io
  repository: nim/meta/llama-3.1-8b-instruct
  tag: "latest"
  pullPolicy: IfNotPresent

# NVIDIA NGC Credentials
# Sign up at: https://catalog.ngc.nvidia.com/
imagePullSecrets:
  - name: ngc-secret

ngc:
  # Replace with your NGC API Key
  apiKey: "<YOUR_NGC_API_KEY>"
  # NGC registry credentials
  registry: nvcr.io
  username: "$oauthtoken"

# Model configuration
model:
  name: "meta/llama-3.1-8b-instruct"
  # OCI Object Storage for model caching (optional but recommended)
  cache:
    enabled: true
    storageClass: "oci-bv"
    size: "50Gi"

# Resource allocation - optimized for VM.GPU.A10.1
resources:
  limits:
    nvidia.com/gpu: 1  # 1 GPU per pod
    memory: "32Gi"
    cpu: "8"
  requests:
    nvidia.com/gpu: 1
    memory: "24Gi"
    cpu: "4"

# Service configuration
service:
  type: LoadBalancer  # Use LoadBalancer for external access
  port: 8000
  targetPort: 8000
  annotations:
    # OCI Load Balancer annotations
    service.beta.kubernetes.io/oci-load-balancer-shape: "flexible"
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: "10"
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: "10"

# Environment variables for NIM
env:
  - name: NIM_CACHE_PATH
    value: "/model-cache"
  - name: NIM_MODEL_PROFILE
    value: "auto"  # Auto-detect optimal settings
  - name: NGC_API_KEY
    valueFrom:
      secretKeyRef:
        name: nvidia-nim-ngc-api
        key: NGC_API_KEY

# Persistent storage for model cache
persistence:
  enabled: true
  storageClass: "oci-bv"  # OCI Block Volume
  accessMode: ReadWriteOnce
  size: 50Gi
  mountPath: /model-cache

# Pod security and node affinity
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false

# Node selector for GPU nodes
nodeSelector:
  nvidia.com/gpu.product: NVIDIA-A10

# Tolerations for GPU nodes
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Affinity rules
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: nvidia.com/gpu.present
              operator: In
              values:
                - "true"

# Topology spread constraints for HA
topologySpreadConstraints:
  enabled: true
  maxSkew: 1
  topologyKey: topology.kubernetes.io/zone
  whenUnsatisfiable: DoNotSchedule

# Readiness and liveness probes
readinessProbe:
  httpGet:
    path: /v1/health/ready
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

livenessProbe:
  httpGet:
    path: /v1/health/live
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

# Autoscaling (disabled by default for cost optimization)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8000"
  prometheus.io/path: "/metrics"

# Service account
serviceAccount:
  create: true
  annotations:
    # OCI service account annotations for instance principals
    oci.oraclecloud.com/principal-type: "instance"
  name: "nim-service-account"

# Additional labels
labels:
  app.kubernetes.io/name: nvidia-nim
  app.kubernetes.io/component: inference
  app.kubernetes.io/part-of: nim-deployment

# Cost optimization settings
costOptimization:
  # Enable this to automatically scale down during off-hours
  scheduleDowntime: false
  # Preemptible/Spot instance (if supported by node pool)
  spotInstance: false

